{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze and evaluate optimization output - TRAINING PROTOCOLS\n",
    "\n",
    "This final notebook uses the `runs.pkl` file created in notebook 2 and it analyzes:\n",
    "\n",
    "- the distance between different feature sets in the parameter space\n",
    "- the distance between different feature sets in the feature space - training protocols\n",
    "- the distance between different feature sets in the extracellular signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import bluepyopt as bpopt\n",
    "import bluepyopt.ephys as ephys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import MEAutility as mu\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import kruskal, mannwhitneyu, wilcoxon\n",
    "\n",
    "import multimodalfitting as mf\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = True\n",
    "figure_folder = Path(\".\") / \"figures_hay_ais\"\n",
    "\n",
    "figure_folder = Path(\"/Users/abuccino/Documents/Submissions/papers/multimodal/hay_ais\") / \"opt\"\n",
    "\n",
    "if save_fig:\n",
    "    figure_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\"soma\": \"C0\",\n",
    "               \"all\": \"C1\",\n",
    "               \"sections\": \"C2\",\n",
    "               \"single\": \"C3\"}\n",
    "feature_sets = {\"soma\": \"soma\",\n",
    "                \"all\": \"extra\",\n",
    "                \"sections\": \"extra\",\n",
    "                \"single\": \"extra\"}\n",
    "figsize = (10, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT params and optimization output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "model_name = \"hay_ais\"\n",
    "probe_type = \"planar\" # linear \n",
    "\n",
    "cell_models_folder = base_dir / \"cell_models\"\n",
    "model_folder = cell_models_folder / model_name\n",
    "probe_file = model_folder / \"fitting\" / \"efeatures\" / \"probe_BPO.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this with folder containing your pkl file\n",
    "results_date = '220214'  # '211124' '220111' # \n",
    "result_folder = base_dir / \"results\" / results_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                    release=False)\n",
    "cell_release = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                            release=True)\n",
    "\n",
    "probe = mf.define_electrode(probe_file=probe_file)\n",
    "\n",
    "param_names = [param.name for param in cell.params.values() if not param.frozen]\n",
    "\n",
    "params_release = {}\n",
    "for param in cell_release.params_by_names(param_names):\n",
    "    params_release[param.name] = param.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_for_eap = \"IDrest_300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_name = \"runs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-posthocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(result_folder / pkl_file_name, 'rb'))\n",
    "df_optimization = pd.DataFrame(data)\n",
    "df_model = df_optimization.query(f\"model == '{model_name}'\")\n",
    "# set strategy column\n",
    "df_model.loc[:, \"strategy\"] = df_model[\"extra_strategy\"].values.copy()\n",
    "df_model.loc[df_model[\"feature_set\"] == \"soma\", \"strategy\"] = \"soma\"\n",
    "\n",
    "if (result_folder / \"opt_results.pkl\").is_file():\n",
    "    with open(result_folder / \"opt_results.pkl\", 'rb') as f:\n",
    "        opt_results_all = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "min_evals = 3000\n",
    "\n",
    "keep_idxs = []\n",
    "for idx, row in df_model.iterrows():\n",
    "    if max(row[\"nevals\"]) > min_evals:\n",
    "        keep_idxs.append(idx)\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=colors_dict[row[\"strategy\"]],\n",
    "                ls='-', \n",
    "                lw=0.8,\n",
    "                alpha=0.75)\n",
    "    else:\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=colors_dict[row[\"strategy\"]],\n",
    "                ls='--', \n",
    "                lw=0.5,\n",
    "                alpha=0.75)\n",
    "\n",
    "ax.set_title(\"Min fitness\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xlabel(\"Neval\")\n",
    "ax.set_ylabel(\"Min fitness\")\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load protocols and original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = mf.utils.get_extra_kwargs()\n",
    "extra_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_extra = mf.create_evaluator(\n",
    "    model_name=model_name,\n",
    "    feature_set=\"extra\",\n",
    "    extra_strategy=\"all\",\n",
    "    protocols_with_lfp=\"IDrest_300\",\n",
    "    **extra_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check num features\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    extra_strategy = strategy if strategy in [\"all\", \"single\", \"sections\"] else None\n",
    "    eva = mf.create_evaluator(\n",
    "        model_name=model_name,\n",
    "        feature_set=feature_sets[strategy],\n",
    "        extra_strategy=extra_strategy,\n",
    "        protocols_with_lfp=\"IDrest_300\",\n",
    "        **extra_kwargs\n",
    "    )\n",
    "    print(f\"Strategy {strategy} --> num features {len(eva.fitness_calculator.objectives)}\")\n",
    "    \n",
    "print(f\"Validation: --> num features {len(eva_extra.fitness_calculator.objectives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute release responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_release = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), param_values=params_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_release = mf.utils.calculate_eap(responses=responses_release, protocols=eva_extra.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap, **extra_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in eva_extra.fitness_calculator.objectives:\n",
    "    print(obj.features[0].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_release = {}\n",
    "for i in tqdm(np.arange(len(eva_extra.fitness_calculator.objectives)), \n",
    "              desc=\"computing features\"):\n",
    "    obj = eva_extra.fitness_calculator.objectives[i]\n",
    "    features_release[obj.features[0].name] = {}\n",
    "    if len(obj.features) == 1:\n",
    "        feat = obj.features[0]\n",
    "        feat_value = feat.calculate_feature(responses_release)\n",
    "        feat_score = feat.calculate_score(responses_release)\n",
    "        if feat_value is None:\n",
    "            print(f\"{feat.name} cannot be computed: skipping\")\n",
    "            continue\n",
    "        features_release[feat.name][\"value\"] = feat_value\n",
    "    else:\n",
    "        print(f\"More than one feature for objective: {obj.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gt_intra = mf.plot_responses(responses_release, color=\"k\", return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mf.plot_eap(responses_release, eva_extra.fitness_protocols, probe,\n",
    "                 protocol_name=protocol_for_eap, color=\"k\")\n",
    "fig_gt_extra = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_gt_intra.savefig(figure_folder / \"gt_intra.pdf\", transparent=True)\n",
    "    fig_gt_extra.savefig(figure_folder / \"gt_extra.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and plot best responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_value = 50\n",
    "opt_results_all = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for strategy in np.unique(df_model.strategy):\n",
    "#     opt_results[strategy] = {}\n",
    "#     opt_df = df_model.query(f\"strategy == '{strategy}'\")\n",
    "#     # get best index\n",
    "#     best_idx = np.argmin([row[\"logbook\"].select(\"min\")[-1] for idx, row in opt_df.iterrows()])\n",
    "#     best_idx_old = np.argmin(opt_df.best_fitness)\n",
    "#     params_sample = opt_df.iloc[best_idx]\n",
    "#     params_dict = {k: v for k, v in zip(param_names, params_sample.best_params)}\n",
    "#     opt_results[strategy][\"best_fitness\"] = params_sample.best_fitness\n",
    "#     opt_results[strategy][\"best_params\"] = params_dict\n",
    "#     print(f\"{strategy} --  best fitness: {params_sample.best_fitness}\")\n",
    "#     print(\"best\", best_idx, \"best absolute\", best_idx_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in np.unique(df_model.strategy):\n",
    "    print(f\"Simulating best '{strategy}'\")\n",
    "    # simulate all responses\n",
    "    opt_df = df_model.query(f\"strategy == '{strategy}'\")\n",
    "    opt_results_all[strategy] = {}\n",
    "    \n",
    "    all_responses = {}\n",
    "    all_eaps = {}\n",
    "    all_params = {}\n",
    "    for idx, row in opt_df.iterrows():\n",
    "        seed = row.seed\n",
    "        print(\"\\tSeed\", row.seed)\n",
    "        population = row.population\n",
    "        scores = [sum(pop.fitness.values) for pop in population]\n",
    "        best_individual_idx = np.argmin(scores)\n",
    "        params = population[best_individual_idx]\n",
    "        params_dict = {k: v for k, v in zip(param_names, params)}\n",
    "        all_params[seed] = params_dict\n",
    "        responses_seed = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), \n",
    "                                                 param_values=params_dict)\n",
    "        all_responses[seed] = responses_seed\n",
    "        eap_seed = mf.utils.calculate_eap(responses=responses_seed, protocols=eva_extra.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap, **extra_kwargs)\n",
    "        all_eaps[seed] = eap_seed\n",
    "    opt_results_all[strategy][\"eaps\"] = all_eaps\n",
    "    opt_results_all[strategy][\"responses\"] = all_responses\n",
    "    opt_results_all[strategy][\"params\"] = all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in opt_results_all:\n",
    "    print(strategy)\n",
    "    opt_results_all[strategy][\"fitness\"] = {}\n",
    "    for seed, responses in opt_results_all[strategy][\"responses\"].items():\n",
    "        extra_fitness = 0\n",
    "        intra_fitness = 0\n",
    "        for i in tqdm(np.arange(len(eva_extra.fitness_calculator.objectives)), \n",
    "                      desc=f\"computing features {strategy}\"):\n",
    "            obj = eva_extra.fitness_calculator.objectives[i]\n",
    "            feat = obj.features[0]\n",
    "            features_best[feat.name] = {}\n",
    "            if len(obj.features) == 1:\n",
    "                feat_value = obj.features[0].calculate_feature(responses)\n",
    "                feat_score = obj.features[0].calculate_score(responses)\n",
    "                if \"MEA\" in feat.name:\n",
    "                    extra_fitness += feat_score\n",
    "                else:\n",
    "                    intra_fitness += feat_score\n",
    "        opt_results_all[strategy][\"fitness\"][seed] = {\"intra\": intra_fitness, \"extra\": extra_fitness, \n",
    "                                                      \"total\": intra_fitness + extra_fitness}\n",
    "        print(\"seed\", seed)\n",
    "        print(\"\\tINTRA\", intra_fitness)\n",
    "        print(\"\\tEXTRA\", extra_fitness)\n",
    "        print(\"\\tTOTAL\", intra_fitness + extra_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_array = []\n",
    "strategy_array = []\n",
    "intra_score_array = []\n",
    "extra_score_array = []\n",
    "total_score_array = []\n",
    "\n",
    "for strategy in opt_results_all:\n",
    "    for seed, fitness in opt_results_all[strategy][\"fitness\"].items():\n",
    "        seeds_array.append(seed)\n",
    "        strategy_array.append(strategy)\n",
    "        intra_score_array.append(fitness[\"intra\"])\n",
    "        extra_score_array.append(fitness[\"extra\"])\n",
    "        total_score_array.append(fitness[\"total\"])\n",
    "df_fitness = pd.DataFrame({\"seed\": seeds_array, \"strategy\": strategy_array,\n",
    "                           \"intra_score\": intra_score_array, \"extra_score\": extra_score_array, \n",
    "                           \"total_score\": total_score_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_extras = df_fitness.iloc[df_fitness.groupby(\"strategy\")[\"extra_score\"].idxmin()]\n",
    "best_extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fitness.sort_values(['intra_score', 'extra_score'],ascending=True).groupby('strategy').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_intra_seeds, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_fitness, x=\"strategy\", y=\"intra_score\", order=order, ax=ax)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "ax.set_title(\"Intracellular\\n(10 seeds)\", fontsize=20)\n",
    "\n",
    "fig_extra_seeds, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_fitness, x=\"strategy\", y=\"extra_score\", order=order)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "ax.set_title(\"Extracellular\\n(10 seeds)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_intra_seeds.savefig(figure_folder / \"intra_seed.pdf\")\n",
    "    fig_extra_seeds.savefig(figure_folder / \"extra_seed.pdf\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in best_extras.iterrows():\n",
    "    strategy = row[\"strategy\"]\n",
    "    seed = row[\"seed\"]\n",
    "    print(\"Strategy\", strategy, \"best seed\", seed)\n",
    "    responses = opt_results_all[strategy][\"responses\"][seed]\n",
    "    eap = opt_results_all[strategy][\"eaps\"][seed]\n",
    "    params = opt_results_all[strategy][\"params\"][seed]\n",
    "    opt_results_all[strategy][\"best_seed\"] = seed\n",
    "    opt_results_all[strategy][\"best_responses\"] = responses\n",
    "    opt_results_all[strategy][\"best_eap\"] = eap\n",
    "    opt_results_all[strategy][\"best_params\"] = params\n",
    "    eap_release_norm = eap_release / np.ptp(np.abs(eap_release), 1, keepdims=True)\n",
    "    eap_norm = eap / np.ptp(np.abs(eap), 1, keepdims=True)\n",
    "    eap_dist = np.sum(np.abs(eap_release_norm.ravel() - eap_norm.ravel()))\n",
    "    opt_results[strategy][\"best_eap_dist\"] = eap_dist\n",
    "    print(eap_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in np.unique(df_model.strategy):\n",
    "    responses = opt_results_all[strategy][\"best_responses\"]\n",
    "    features_best = {}\n",
    "    for obj in eva_extra.fitness_calculator.objectives:\n",
    "        feat = obj.features[0]\n",
    "        features_best[feat.name] = {}\n",
    "        if len(obj.features) == 1:\n",
    "            feat_value = obj.features[0].calculate_feature(responses)\n",
    "            if feat_value is None:\n",
    "                feat_value = max_feature_value\n",
    "            features_best[feat.name][\"value\"] = feat_value\n",
    "            if \"MEA\" not in feat.name:\n",
    "                feat_score = np.abs(features_release[feat.name][\"value\"] - feat_value) / feat.exp_std\n",
    "            else:\n",
    "                feat_score = np.abs(distance.cosine(features_release[feat.name][\"value\"], feat_value))\n",
    "            features_best[feat.name][\"score\"] = feat_score\n",
    "        else:\n",
    "            print(f\"More than one feature for objective: {obj.name}\")\n",
    "    opt_results_all[strategy][\"features\"] = features_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "figs_intra = {}\n",
    "figs_extra = {}\n",
    "protocols_to_plot = [\"APWaveform_290\", \"IDrest_250\", \"IV_-100\"]\n",
    "titles = protocols_to_plot\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    responses_to_plot = [responses_release, opt_results_all[strategy][\"best_responses\"]]\n",
    "    colors = [\"k\", colors_dict[strategy]]\n",
    "    labels = [\"GT\", strategy.upper()]\n",
    "    fig_intra = mf.plot_multiple_responses(responses_to_plot, \n",
    "                                           protocol_names=protocols_to_plot,\n",
    "                                           colors=colors, \n",
    "                                           titles=titles,\n",
    "                                           return_fig=True, \n",
    "                                           labels=labels)\n",
    "    ax_extra = mf.plot_multiple_eaps(responses_to_plot, \n",
    "                                     eva_extra.fitness_protocols, probe,\n",
    "                                     protocol_name=protocol_for_eap, \n",
    "                                     colors=colors, labels=labels, norm=True)\n",
    "    fig_extra = ax_extra.get_figure()\n",
    "    figs_intra[strategy] = fig_intra\n",
    "    figs_extra[strategy] = fig_extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_folder / \"opt_results.pkl\", 'wb') as f:\n",
    "    pickle.dump(opt_results_all, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in opt_results:\n",
    "    print(f\"Distance {strategy}: {opt_results[strategy]['eap_dist']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    for strategy, fig in figs_intra.items():\n",
    "        fig.savefig(figure_folder / f\"{strategy}_intra.pdf\", transparent=True)\n",
    "    for strategy, fig in figs_extra.items():\n",
    "        fig.savefig(figure_folder / f\"{strategy}_extra.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare best-fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame.from_dict(opt_results, orient=\"index\")\n",
    "df_test[\"strategy\"] = df_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare parameters \n",
    "\n",
    "Here we normalize the parameters based on the boundaries and compute the relative difference to GT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_json = model_folder / \"parameters.json\"\n",
    "\n",
    "with param_json.open() as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "param_boundaries = {}\n",
    "for param in params:\n",
    "    if \"bounds\" in param:\n",
    "        if isinstance(param['sectionlist'], list):\n",
    "            for sec in param['sectionlist']:\n",
    "                param_boundaries[f\"{param['param_name']}_{sec}\"] = param[\"bounds\"]\n",
    "        else:\n",
    "            sec = param['sectionlist']\n",
    "            param_boundaries[f\"{param['param_name']}_{sec}\"] = param[\"bounds\"]\n",
    "\n",
    "# scale params_release by boundaries\n",
    "params_release_norm = {}\n",
    "for param_name, param_val in params_release.items():\n",
    "    bounds = param_boundaries[param_name]\n",
    "    param_norm = (param_val - bounds[0]) / (bounds[1] - bounds[0])\n",
    "    params_release_norm[param_name] = param_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_release_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_array = []\n",
    "param_name_array = []\n",
    "param_value_array = []\n",
    "param_norm_array = []\n",
    "release_value_array = []\n",
    "release_norm_array = []\n",
    "diff_with_release_array = []\n",
    "section_array = []\n",
    "\n",
    "for i, (index, opt) in enumerate(df_test.iterrows()):\n",
    "    params_dict = opt.best_params\n",
    "    \n",
    "    for param_name, param_value in params_dict.items():\n",
    "        feature_set_array.append(opt.strategy)\n",
    "        param_name_array.append(param_name)\n",
    "        param_value_array.append(param_value)\n",
    "        section = param_name.split(\"_\")[-1]\n",
    "        if section == \"segment\":\n",
    "            section = \"ais\"\n",
    "        section_array.append(section)\n",
    "        release_value_array.append(params_release[param_name])\n",
    "        release_norm_array.append(params_release_norm[param_name])\n",
    "        # compute norm value\n",
    "        bounds = param_boundaries[param_name]\n",
    "        param_norm = (param_value - bounds[0]) / (bounds[1] - bounds[0])\n",
    "        param_norm_array.append(param_norm)\n",
    "        diff_with_release = np.abs(param_value - params_release[param_name]) / params_release[param_name]\n",
    "#         diff_with_release_array.append(abs(param_norm - params_release_norm[param_name]))\n",
    "        diff_with_release_array.append(diff_with_release)\n",
    "\n",
    "        \n",
    "df_params = pd.DataFrame({\"strategy\": feature_set_array, \"param_name\": param_name_array,\n",
    "                          \"param_value\": param_value_array, \"param_norm\": param_norm_array, \n",
    "                          \"release_value\": release_value_array, \"release_norm\": release_norm_array,\n",
    "                          \"diff_release\": diff_with_release_array, \"section\": section_array}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_full = [\"soma\", \"all\", \"sections\", \"single\"]\n",
    "order = []\n",
    "for strategy in order_full:\n",
    "    if strategy in opt_results:\n",
    "        order.append(strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall parameter diff\n",
    "fig, ax = plt.subplots(figsize=figsize)\n",
    "sns.boxenplot(data=df_params, y=\"strategy\", x=\"diff_release\", ax=ax, order=order)\n",
    "ax.set_xlabel(\"Relative error (%)\", fontsize=12)\n",
    "ax.set_ylabel(\"Straregy\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"All params\", fontsize=15)\n",
    "ax.set_xlim(0, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for section in np.unique(df_params.section):\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "    df_section = df_params.query(f\"section == '{section}'\")\n",
    "    sns.barplot(data=df_section, y=\"param_name\", x=\"diff_release\", hue=\"strategy\",\n",
    "                orientation=\"horizontal\", ax=ax1, hue_order=order)\n",
    "    ax1.set_xlabel(\"Norm. param difference\", fontsize=12)\n",
    "    ax1.set_ylabel(\"Param\", fontsize=12)\n",
    "    ax1.spines[\"top\"].set_visible(False)\n",
    "    ax1.spines[\"right\"].set_visible(False)\n",
    "    ax1.set_title(f\"{section} params\", fontsize=15)\n",
    "    \n",
    "    fig, ax2 = plt.subplots(figsize=figsize)\n",
    "    sns.boxplot(data=df_section, y=\"strategy\", x=\"diff_release\", ax=ax2, order=order)\n",
    "    ax2.set_xlabel(\"Norm. param difference\", fontsize=12)\n",
    "    ax2.set_ylabel(\"Strategy\", fontsize=12)\n",
    "    ax2.spines[\"top\"].set_visible(False)\n",
    "    ax2.spines[\"right\"].set_visible(False)\n",
    "    ax2.set_title(f\"{section} params - ALL\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_array = []\n",
    "feature_set_array = []\n",
    "feature_score_array = []\n",
    "feature_type_array = []\n",
    "\n",
    "for strategy, res in opt_results.items():\n",
    "    feats = res[\"features\"]\n",
    "    for feat_name, feat_dict in feats.items():\n",
    "        feature_set_array.append(strategy)\n",
    "        feature_name_array.append(feat_name)\n",
    "        if \"MEA\" not in feat_name:\n",
    "            feature_type_array.append(\"intra\")\n",
    "        else:\n",
    "            feature_type_array.append(\"extra\")\n",
    "        feature_score_array.append(feat_dict[\"score\"])\n",
    "        \n",
    "df_feats = pd.DataFrame({\"feature_set\": feature_set_array, \"feat_name\": feature_name_array,\n",
    "                         \"feat_score\": feature_score_array, \"feature_type\": feature_type_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_feat_intra, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "sns.boxplot(data=df_feats.query(\"feature_type == 'intra'\"), y=\"feature_set\", x=\"feat_score\", \n",
    "            order=order, ax=ax)\n",
    "ax.set_ylabel(\"Feature scores (intracellular)\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Intracellular features\", fontsize=15)\n",
    "\n",
    "fig_feat_extra, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "sns.boxplot(data=df_feats.query(\"feature_type == 'extra'\"), \n",
    "            y=\"feature_set\", x=\"feat_score\", order=order, ax=ax)\n",
    "ax.set_ylabel(\"Feature scores (extracellular)\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Extracellular features\", fontsize=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats.query(\"feature_type == 'intra'\").groupby(\"feature_set\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats.query(\"feature_type == 'extra'\").groupby(\"feature_set\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, wilcoxon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_intra = df_feats.query(\"feature_type == 'intra'\")\n",
    "df_extra = df_feats.query(\"feature_type == 'extra'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_soma = df_intra.query(\"feature_set == 'soma'\")[\"feat_score\"]\n",
    "intra_sections = df_intra.query(\"feature_set == 'sections'\")[\"feat_score\"]\n",
    "intra_all = df_intra.query(\"feature_set == 'all'\")[\"feat_score\"]\n",
    "intra_single = df_intra.query(\"feature_set == 'single'\")[\"feat_score\"]\n",
    "\n",
    "extra_soma = df_extra.query(\"feature_set == 'soma'\")[\"feat_score\"]\n",
    "extra_sections = df_extra.query(\"feature_set == 'sections'\")[\"feat_score\"]\n",
    "extra_all = df_extra.query(\"feature_set == 'all'\")[\"feat_score\"]\n",
    "extra_single = df_extra.query(\"feature_set == 'single'\")[\"feat_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intra - Sections VS SOMA:\", wilcoxon(intra_sections, intra_soma))\n",
    "print(\"Intra - All VS SOMA:\", wilcoxon(intra_all, intra_soma))\n",
    "print(\"Intra - Single VS SOMA:\", wilcoxon(intra_single, intra_soma))\n",
    "# print(\"Intra - All VS Sections:\", wilcoxon(intra_sections, intra_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Extra - Sections VS SOMA:\", wilcoxon(extra_sections, extra_soma))\n",
    "print(\"Extra - All VS SOMA:\", wilcoxon(extra_all, extra_soma))\n",
    "print(\"Extra - Single VS SOMA:\", wilcoxon(extra_single, extra_soma))\n",
    "# print(\"Extra - All VS Sections:\", wilcoxon(extra_sections, extra_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_feat_intra.savefig(figure_folder / \"feat_intra.pdf\")\n",
    "    fig_feat_extra.savefig(figure_folder / \"feat_extra.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare EAP distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"eap_dist\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cos, ax = plt.subplots()\n",
    "sns.barplot(data=df_test, x=\"strategy\", y=\"eap_dist\", order=order, ax=ax)\n",
    "ax.set_ylabel(\"Cosine distance\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Extracellular difference\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_cos.savefig(figure_folder / \"eap_dist.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
