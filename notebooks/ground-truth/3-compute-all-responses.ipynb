{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze and evaluate optimization output - TRAINING PROTOCOLS\n",
    "\n",
    "This final notebook uses the `runs.pkl` file created in notebook 2 and it analyzes:\n",
    "\n",
    "- the distance between different feature sets in the parameter space\n",
    "- the distance between different feature sets in the feature space - training protocols\n",
    "- the distance between different feature sets in the extracellular signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import bluepyopt as bpopt\n",
    "import bluepyopt.ephys as ephys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import MEAutility as mu\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import kruskal, mannwhitneyu, wilcoxon\n",
    "\n",
    "import multimodalfitting as mf\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = True\n",
    "figure_folder = Path(\"..\") / \"figures\"\n",
    "\n",
    "if save_fig:\n",
    "    figure_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\"soma\": \"C0\",\n",
    "               \"all\": \"C1\",\n",
    "               \"sections\": \"C2\",\n",
    "               \"single\": \"C3\"}\n",
    "feature_sets = {\"soma\": \"soma\",\n",
    "                \"all\": \"extra\",\n",
    "                \"sections\": \"extra\",\n",
    "                \"single\": \"extra\"}\n",
    "figsize = (10, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT params and optimization output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "model_name = \"hay_ais\"\n",
    "probe_type = \"planar\" # linear \n",
    "\n",
    "cell_models_folder = base_dir / \"cell_models\"\n",
    "model_folder = cell_models_folder / model_name\n",
    "probe_file = model_folder / \"fitting\" / \"efeatures\" / \"probe_BPO.json\"\n",
    "pkl_file_name = \"runs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_folder = Path(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dates = [r.name for r in (base_dir / \"results\").iterdir()]\n",
    "# use latest results\n",
    "results_date = max(result_dates)\n",
    "result_folder = base_dir / \"results\" / results_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                    release=False)\n",
    "cell_release = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                            release=True)\n",
    "\n",
    "probe = mf.define_electrode(probe_file=probe_file)\n",
    "\n",
    "param_names = [param.name for param in cell.params.values() if not param.frozen]\n",
    "\n",
    "params_release = {}\n",
    "for param in cell_release.params_by_names(param_names):\n",
    "    params_release[param.name] = param.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_for_eap = \"IDrest_300\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(result_folder / pkl_file_name, 'rb'))\n",
    "\n",
    "df_optimization = pd.DataFrame(data)\n",
    "df_model = df_optimization.query(f\"model == '{model_name}'\")\n",
    "\n",
    "opt_results_training = None\n",
    "results_file = f\"all_responses_{model_name}.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "min_evals = 3000\n",
    "\n",
    "keep_idxs = []\n",
    "for idx, row in df_model.iterrows():\n",
    "    if max(row[\"nevals\"]) > min_evals:\n",
    "        keep_idxs.append(idx)\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=colors_dict[row[\"strategy\"]],\n",
    "                ls='-', \n",
    "                lw=0.8,\n",
    "                alpha=0.75)\n",
    "    else:\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=colors_dict[row[\"strategy\"]],\n",
    "                ls='--', \n",
    "                lw=0.5,\n",
    "                alpha=0.75)\n",
    "\n",
    "ax.set_title(\"Min fitness\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xlabel(\"Neval\")\n",
    "ax.set_ylabel(\"Min fitness\")\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load protocols and original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = mf.utils.get_extra_kwargs()\n",
    "extra_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_extra = mf.create_evaluator(\n",
    "    model_name=model_name,\n",
    "    strategy=\"all\",\n",
    "    protocols_with_lfp=\"IDrest_300\",\n",
    "    **extra_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check num features\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    eva = mf.create_evaluator(\n",
    "        model_name=model_name,\n",
    "        strategy=strategy,\n",
    "        protocols_with_lfp=\"IDrest_300\",\n",
    "        **extra_kwargs\n",
    "    )\n",
    "    print(f\"Strategy {strategy} --> num features {len(eva.fitness_calculator.objectives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute release responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_release = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), param_values=params_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_release = mf.utils.calculate_eap(responses=responses_release, protocols=eva_extra.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap, **extra_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_release = {}\n",
    "for i in tqdm(np.arange(len(eva_extra.fitness_calculator.objectives)), \n",
    "              desc=\"computing features\"):\n",
    "    obj = eva_extra.fitness_calculator.objectives[i]\n",
    "    features_release[obj.features[0].name] = {}\n",
    "    if len(obj.features) == 1:\n",
    "        feat = obj.features[0]\n",
    "        feat_value = feat.calculate_feature(responses_release)\n",
    "        feat_score = feat.calculate_score(responses_release)\n",
    "        if feat_value is None:\n",
    "            print(f\"{feat.name} cannot be computed: skipping\")\n",
    "            continue\n",
    "        features_release[feat.name][\"value\"] = feat_value\n",
    "    else:\n",
    "        print(f\"More than one feature for objective: {obj.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gt_intra = mf.plot_responses(responses_release, color=\"k\", return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = mf.plot_eap(responses_release, eva_extra.fitness_protocols, probe,\n",
    "                 protocol_name=protocol_for_eap, color=\"k\")\n",
    "fig_gt_extra = ax.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_gt_intra.savefig(figure_folder / \"gt_intra.pdf\", transparent=True)\n",
    "    fig_gt_extra.savefig(figure_folder / \"gt_extra.pdf\", transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and plot best responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results_training = {}\n",
    "\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    print(f\"Simulating best '{strategy}'\")\n",
    "    # simulate all responses\n",
    "    opt_df = df_model.query(f\"strategy == '{strategy}'\")\n",
    "    opt_results_training[strategy] = {}\n",
    "\n",
    "    all_responses = {}\n",
    "    all_eaps = {}\n",
    "    all_params = {}\n",
    "    for idx, row in opt_df.iterrows():\n",
    "        seed = row.seed\n",
    "        print(\"\\tSeed\", row.seed)\n",
    "        population = row.population\n",
    "        scores = [sum(pop.fitness.values) for pop in population]\n",
    "        best_individual_idx = np.argmin(scores)\n",
    "        params = population[best_individual_idx]\n",
    "        params_dict = {k: v for k, v in zip(param_names, params)}\n",
    "        all_params[seed] = params_dict\n",
    "        responses_seed = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), \n",
    "                                                 param_values=params_dict)\n",
    "        all_responses[seed] = responses_seed\n",
    "        eap_seed = mf.utils.calculate_eap(responses=responses_seed, protocols=eva_extra.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap, **extra_kwargs)\n",
    "        all_eaps[seed] = eap_seed\n",
    "    opt_results_training[strategy][\"eaps\"] = all_eaps\n",
    "    opt_results_training[strategy][\"responses\"] = all_responses\n",
    "    opt_results_training[strategy][\"params\"] = all_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for strategy in opt_results_training:\n",
    "    features_best = {}\n",
    "    opt_results_training[strategy][\"fitness\"] = {}\n",
    "    for seed, responses in opt_results_training[strategy][\"responses\"].items():\n",
    "        extra_fitness = 0\n",
    "        intra_fitness = 0\n",
    "        for i in tqdm(np.arange(len(eva_extra.fitness_calculator.objectives)), \n",
    "                      desc=f\"computing features {strategy}\"):\n",
    "            obj = eva_extra.fitness_calculator.objectives[i]\n",
    "            feat = obj.features[0]\n",
    "            features_best[feat.name] = {}\n",
    "            if len(obj.features) == 1:\n",
    "                feat_value = obj.features[0].calculate_feature(responses)\n",
    "                feat_score = obj.features[0].calculate_score(responses)\n",
    "                if \"MEA\" in feat.name:\n",
    "                    extra_fitness += feat_score\n",
    "                else:\n",
    "                    intra_fitness += feat_score\n",
    "        opt_results_training[strategy][\"fitness\"][seed] = {\"intra\": intra_fitness, \"extra\": extra_fitness, \n",
    "                                                           \"total\": intra_fitness + extra_fitness}\n",
    "        print(\"seed\", seed)\n",
    "        print(\"\\tINTRA\", intra_fitness)\n",
    "        print(\"\\tEXTRA\", extra_fitness)\n",
    "        print(\"\\tTOTAL\", intra_fitness + extra_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best responses are the solutions that minimize intra_score\n",
    "best_extras = df_fitness.loc[df_fitness.groupby(\"strategy\")[\"intra_score\"].idxmin()]\n",
    "print(best_extras)\n",
    "\n",
    "for idx, row in best_extras.iterrows():\n",
    "    strategy = row[\"strategy\"]\n",
    "    seed = row[\"seed\"]\n",
    "    print(\"Strategy\", strategy, \"best seed\", seed)\n",
    "    responses = opt_results_training[strategy][\"responses\"][seed]\n",
    "    eap = opt_results_training[strategy][\"eaps\"][seed]\n",
    "    params = opt_results_training[strategy][\"params\"][seed]\n",
    "    opt_results_training[strategy][\"best_seed\"] = seed\n",
    "    opt_results_training[strategy][\"best_responses\"] = responses\n",
    "    opt_results_training[strategy][\"best_eap\"] = eap\n",
    "    opt_results_training[strategy][\"best_params\"] = params\n",
    "    eap_release_norm = eap_release / np.ptp(np.abs(eap_release), 1, keepdims=True)\n",
    "    eap_norm = eap / np.ptp(np.abs(eap), 1, keepdims=True)\n",
    "    eap_dist = np.sum(np.abs(eap_release_norm.ravel() - eap_norm.ravel()))\n",
    "    opt_results_training[strategy][\"best_eap_dist\"] = eap_dist\n",
    "    print(f\"EAP distance: {eap_dist}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_folder / results_file, 'wb') as f:\n",
    "    pickle.dump(opt_results_training, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
