{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Analyze and evaluate optimization output\n",
    "\n",
    "This final notebook uses the `runs.pkl` file created in notebook 2 and it analyzes the optimization performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import bluepyopt as bpopt\n",
    "import bluepyopt.ephys as ephys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import MEAutility as mu\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import kruskal, mannwhitneyu, wilcoxon\n",
    "\n",
    "import multimodalfitting as mf\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = True\n",
    "figure_folder = Path(\"..\") / \"figures\"\n",
    "\n",
    "if save_fig:\n",
    "    figure_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\"soma\": \"C0\",\n",
    "               \"all\": \"C1\",\n",
    "               \"sections\": \"C2\",\n",
    "               \"single\": \"C3\"}\n",
    "feature_sets = {\"soma\": \"soma\",\n",
    "                \"all\": \"extra\",\n",
    "                \"sections\": \"extra\",\n",
    "                \"single\": \"extra\"}\n",
    "figsize = (10, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT params and optimization output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "model_name = \"hay_ais\"\n",
    "probe_type = \"planar\" # linear \n",
    "\n",
    "cell_models_folder = base_dir / \"cell_models\"\n",
    "model_folder = cell_models_folder / model_name\n",
    "probe_file = model_folder / \"fitting\" / \"efeatures\" / \"probe_BPO.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dates = [r.name for r in (base_dir / \"results\").iterdir()]\n",
    "# use latest results\n",
    "results_date = max(result_dates)\n",
    "result_folder = base_dir / \"results\" / results_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                    release=False)\n",
    "cell_release = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                            release=True)\n",
    "\n",
    "probe = mf.define_electrode(probe_file=probe_file)\n",
    "\n",
    "param_names = [param.name for param in cell.params.values() if not param.frozen]\n",
    "\n",
    "params_release = {}\n",
    "for param in cell_release.params_by_names(param_names):\n",
    "    params_release[param.name] = param.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_for_eap = \"IDrest_300\"\n",
    "protocol_for_eap_val = \"firepattern_120\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_name = \"runs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(result_folder / pkl_file_name, 'rb'))\n",
    "df_optimization = pd.DataFrame(data)\n",
    "df_model = df_optimization.query(f\"model == '{model_name}'\")\n",
    "\n",
    "opt_results_training = None\n",
    "results_file = f\"all_responses_{model_name}.pkl\"\n",
    "if (result_folder / results_file).is_file():\n",
    "    with open(result_folder / results_file, 'rb') as f:\n",
    "        opt_results_training = pickle.load(f)\n",
    "else:\n",
    "    raise Exception(f\"Couldn't fint result file: {results_file}. Run notebook 3a first!\")\n",
    "\n",
    "opt_results_validation = None\n",
    "results_val_file = f\"validation_responses_{model_name}.pkl\"\n",
    "if (result_folder / results_val_file).is_file():\n",
    "    with open(result_folder / results_val_file, 'rb') as f:\n",
    "        opt_results_val = pickle.load(f)\n",
    "        compute_val_responses = False\n",
    "else:\n",
    "    compute_val_responses = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load protocols and original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = mf.utils.get_extra_kwargs()\n",
    "extra_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_used_for_opt = [\"IV_-20\", \"IV_-100\", \"IDrest_150\", \"IDrest_250\", \"IDrest_300\",\n",
    "                          \"APWaveform_260\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_to_exclude = [\"IV\", \"APWaveform\", \"IDrest\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_extra_train = mf.create_evaluator(\n",
    "    model_name=model_name,\n",
    "    strategy=\"all\",\n",
    "    protocols_with_lfp=protocol_for_eap,\n",
    "    all_protocols=False,\n",
    "    **extra_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_extra_val = mf.create_evaluator(\n",
    "    model_name=model_name,\n",
    "    strategy=\"all\",\n",
    "    protocols_with_lfp=protocol_for_eap_val,\n",
    "    all_protocols=True,\n",
    "    exclude_protocols=protocols_to_exclude,\n",
    "    **extra_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All training features --> num features {len(eva_extra_train.fitness_calculator.objectives)}\")\n",
    "print(f\"All validation features --> num features {len(eva_extra_val.fitness_calculator.objectives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute release responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "responses_release_train = eva_extra_train.run_protocols(eva_extra_train.fitness_protocols.values(), \n",
    "                                                        param_values=params_release)\n",
    "t_stop = time.time()\n",
    "print(f\"Simulated responses in {np.round(t_stop - t_start, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()\n",
    "responses_release_val = eva_extra_val.run_protocols(eva_extra_val.fitness_protocols.values(), \n",
    "                                                    param_values=params_release)\n",
    "t_stop = time.time()\n",
    "print(f\"Simulated responses in {np.round(t_stop - t_start, 2)} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eap_release = mf.utils.calculate_eap(responses=responses_release_val, \n",
    "                                     protocols=eva_extra_val.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap_val, **extra_kwargs)\n",
    "\n",
    "# compute extracellular features\n",
    "std_from_mean = 0.05\n",
    "extra_features = mf.efeatures_extraction.compute_extra_features(\n",
    "    eap_release, fs=extra_kwargs[\"fs\"],\n",
    "    upsample=extra_kwargs[\"upsample\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_release = {}\n",
    "for i in tqdm(np.arange(len(eva_extra_val.fitness_calculator.objectives)), \n",
    "              desc=\"computing features\"):\n",
    "    obj = eva_extra_val.fitness_calculator.objectives[i]\n",
    "    features_release[obj.features[0].name] = {}\n",
    "    if len(obj.features) == 1:\n",
    "        feat = obj.features[0]\n",
    "        feat_value = feat.calculate_feature(responses_release_val)\n",
    "        feat_score = feat.calculate_score(responses_release_val)\n",
    "        features_release[feat.name][\"value\"] = feat_value\n",
    "    else:\n",
    "        print(f\"More than one feature for objective: {obj.name}\")\n",
    "\n",
    "num_intra_features = len(features_release)\n",
    "print(f\"Intra features: {num_intra_features}\")\n",
    "# add extra features\n",
    "for efeat_name, feat in extra_features.items():\n",
    "    for chan, feat_val in enumerate(feat):\n",
    "        fature_name = f\"{protocol_for_eap}.MEA.{efeat_name}_{chan}\"\n",
    "        features_release[fature_name] = {}\n",
    "        features_release[fature_name][\"value\"] = feat_val\n",
    "num_extra_features = len(features_release) - num_intra_features\n",
    "print(f\"Extra features: {num_extra_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gt_intra = mf.plot_responses(responses_release_train, color=\"k\", return_fig=True, max_rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find best responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_array = []\n",
    "strategy_array = []\n",
    "intra_score_array = []\n",
    "extra_score_array = []\n",
    "total_score_array = []\n",
    "\n",
    "strategies = [\"soma\", \"all\", \"sections\", \"single\"]\n",
    "              \n",
    "for strategy in strategies:\n",
    "    for seed, fitness in opt_results_training[strategy][\"fitness\"].items():\n",
    "        seeds_array.append(seed)\n",
    "        strategy_array.append(strategy)\n",
    "        intra_score_array.append(fitness[\"intra\"])\n",
    "        extra_score_array.append(fitness[\"extra\"])\n",
    "        total_score_array.append(fitness[\"total\"])\n",
    "df_fitness = pd.DataFrame({\"seed\": seeds_array, \"strategy\": strategy_array,\n",
    "                           \"intra_score\": intra_score_array, \"extra_score\": extra_score_array, \n",
    "                           \"total_score\": total_score_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = [\"soma\", \"all\", \"sections\", \"single\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_intra_seeds, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_fitness, x=\"strategy\", y=\"intra_score\", order=order, ax=ax,\n",
    "            palette=colors_dict)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "ax.set_title(\"Intracellular\\n(10 seeds)\", fontsize=20)\n",
    "\n",
    "fig_extra_seeds, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_fitness, x=\"strategy\", y=\"extra_score\", order=order,\n",
    "            palette=colors_dict)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), fontsize=12)\n",
    "ax.set_title(\"Extracellular\\n(10 seeds)\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_intra_seeds.savefig(figure_folder / \"fig4A-left.pdf\")\n",
    "    fig_extra_seeds.savefig(figure_folder / \"fig4A-right.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_value = 50\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    responses = opt_results_training[strategy][\"best_responses\"]\n",
    "    features_best = {}\n",
    "    for i in tqdm(np.arange(len(eva_extra_val.fitness_calculator.objectives)), \n",
    "                  desc=f\"computing features {strategy}\"):\n",
    "        obj = eva_extra_train.fitness_calculator.objectives[i]\n",
    "        feat = obj.features[0]\n",
    "        features_best[feat.name] = {}\n",
    "        if len(obj.features) == 1:\n",
    "            feat_value = obj.features[0].calculate_feature(responses)\n",
    "            if feat_value is None:\n",
    "                feat_value = max_feature_value\n",
    "            features_best[feat.name][\"value\"] = feat_value\n",
    "            if \"MEA\" not in feat.name:\n",
    "                feat_score = np.abs(features_release[feat.name][\"value\"] - feat_value) / feat.exp_std\n",
    "            else:\n",
    "                feat_score = np.abs(distance.cosine(features_release[feat.name][\"value\"], feat_value))\n",
    "            features_best[feat.name][\"score\"] = feat_score\n",
    "        else:\n",
    "            print(f\"More than one feature for objective: {obj.name}\")\n",
    "    opt_results_training[strategy][\"features\"] = features_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot responses to training protocols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_intra = {}\n",
    "figs_extra = {}\n",
    "protocols_to_plot = [\"APWaveform_290\", \"IDrest_250\", \"IV_-100\"]\n",
    "titles = protocols_to_plot\n",
    "for strategy in np.unique(df_model.strategy):\n",
    "    responses_to_plot = [responses_release_train, opt_results_training[strategy][\"best_responses\"]]\n",
    "    colors = [\"k\", colors_dict[strategy]]\n",
    "    labels = [\"GT\", strategy.upper()]\n",
    "    fig_intra = mf.plot_multiple_responses(responses_to_plot, \n",
    "                                           protocol_names=protocols_to_plot,\n",
    "                                           colors=colors, \n",
    "                                           titles=titles,\n",
    "                                           return_fig=True, \n",
    "                                           labels=labels)\n",
    "    figs_intra[strategy] = fig_intra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and plot validation responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_value = 50\n",
    "opt_results_val = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\"soma\", \"all\", \"sections\", \"single\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_val_responses:\n",
    "    for strategy in strategies:\n",
    "        opt_results_val[strategy] = {}\n",
    "        print(f\"Simulating best '{strategy}' -- seed: {opt_results_training[strategy]['best_seed']}\")\n",
    "        best_params = opt_results_training[strategy][\"best_params\"]\n",
    "        t_start = time.time()\n",
    "        responses = eva_extra_val.run_protocols(eva_extra_val.fitness_protocols.values(), \n",
    "                                                param_values=best_params)\n",
    "        eap = mf.utils.calculate_eap(responses=responses, protocols=eva_extra_val.fitness_protocols, \n",
    "                                     protocol_name=protocol_for_eap_val, **extra_kwargs)\n",
    "        t_stop = time.time()\n",
    "        print(f\"Simulated responses in {np.round(t_stop - t_start, 2)} s\")\n",
    "        eap_release_norm = eap_release / np.ptp(np.abs(eap_release), 1, keepdims=True)\n",
    "        eap_norm = eap / np.ptp(np.abs(eap), 1, keepdims=True)\n",
    "        eap_dist = np.sum(np.abs(eap_release_norm.ravel() - eap_norm.ravel()))\n",
    "        opt_results_val[strategy][\"eap_dist\"] = eap_dist\n",
    "        opt_results_val[strategy][\"responses\"] = responses\n",
    "        opt_results_val[strategy][\"eap\"] = eap    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_objectives = [obj.features[0].name for obj in eva_extra_val.fitness_calculator.objectives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feat_objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compute_val_responses:\n",
    "    for strategy in strategies:\n",
    "        responses = opt_results_val[strategy][\"responses\"]\n",
    "        eap = opt_results_val[strategy][\"eap\"]\n",
    "        extra_features_strategy = mf.efeatures_extraction.compute_extra_features(\n",
    "                                        eap, fs=extra_kwargs[\"fs\"],\n",
    "                                        upsample=extra_kwargs[\"upsample\"])\n",
    "        opt_results_val[strategy][\"extra_features\"] = extra_features_strategy\n",
    "\n",
    "        features_best = {}\n",
    "        feat_release_keys = list(features_release.keys())\n",
    "        for i in tqdm(np.arange(len(feat_release_keys)), desc=f\"computing features {strategy}\"):\n",
    "\n",
    "            feat_name = feat_release_keys[i]\n",
    "            features_best[feat_name] = {}\n",
    "            \n",
    "            release_value = features_release[feat_name][\"value\"]\n",
    "\n",
    "            if feat_name in feat_objectives:\n",
    "                feat = eva_extra_val.fitness_calculator.objectives[feat_objectives.index(feat_name)].features[0]\n",
    "\n",
    "                feat_value = feat.calculate_feature(responses)\n",
    "                if feat_value is None:\n",
    "                    feat_value = max_feature_value\n",
    "\n",
    "                feat_score = np.abs(release_value - feat_value) / np.abs(std_from_mean * release_value)\n",
    "\n",
    "            else:\n",
    "                # extra\n",
    "                _, _, efeat_full = feat_name.split(\".\")\n",
    "                efeat_split = efeat_full.split(\"_\")\n",
    "                chan = int(efeat_split[-1])\n",
    "                efeat = \"_\".join(efeat_split[:-1])\n",
    "\n",
    "                feat_value = extra_features_strategy[efeat][chan]\n",
    "\n",
    "                if release_value != 0:\n",
    "                    feat_score = abs(feat_value - release_value) / abs(std_from_mean * release_value)\n",
    "                else:                    \n",
    "                    feat_score = abs(feat_value - release_value)\n",
    "\n",
    "            features_best[feat_name] = {\"value\": feat_value, \"score\": feat_score}\n",
    "\n",
    "        opt_results_val[strategy][\"features\"] = features_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_to_plot = [\"firepattern_200\", \"HyperDepol_-160\", \"HyperDepol_-40\", \n",
    "                     \"sAHP_250\", \"PosCheops_300\"]\n",
    "titles = protocols_to_plot\n",
    "figs_intra = {}\n",
    "for strategy in strategies:\n",
    "    responses_to_plot = [responses_release_val, opt_results_val[strategy][\"responses\"]]\n",
    "    colors = [\"k\", colors_dict[strategy]]\n",
    "    labels = [\"GT\", strategy.upper()]\n",
    "    fig = mf.plot_multiple_responses(responses_to_plot, \n",
    "                                     colors=colors, return_fig=True, \n",
    "                                     protocol_names=protocols_to_plot,\n",
    "                                     titles=titles,\n",
    "                                     figsize=(7, 12))\n",
    "    figs_intra[strategy] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_extra = {}\n",
    "for strategy in strategies:\n",
    "    responses = opt_results_val[strategy][\"responses\"]\n",
    "    responses_to_plot = [responses_release_val, responses]\n",
    "    colors = [\"k\", colors_dict[strategy]]\n",
    "    labels = [\"GT\", strategy.upper()]\n",
    "    ax_extra = mf.plot_multiple_eaps(responses_to_plot, \n",
    "                                     eva_extra_val.fitness_protocols, probe,\n",
    "                                     protocol_name=protocol_for_eap_val, \n",
    "                                     colors=colors, #labels=labels, \n",
    "                                     norm=True)\n",
    "    fig = ax_extra.get_figure()\n",
    "    figs_extra[strategy] = fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    for strategy in figs_intra.keys():\n",
    "        figs_intra[strategy].savefig(figure_folder / f\"fig4C_{strategy}.png\", dpi=300)\n",
    "        figs_intra[strategy].savefig(figure_folder / f\"fig4C_{strategy}.pdf\")\n",
    "        figs_extra[strategy].savefig(figure_folder / f\"fig4D_{strategy}.png\", dpi=300)        \n",
    "        figs_extra[strategy].savefig(figure_folder / f\"fig4D_{strategy}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare best-fitted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_full = [\"soma\", \"all\", \"sections\", \"single\"]\n",
    "order = []\n",
    "for strategy in order_full:\n",
    "    if strategy in opt_results_val:\n",
    "        order.append(strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_array = []\n",
    "feature_set_array = []\n",
    "feature_score_array = []\n",
    "feature_type_array = []\n",
    "protocol_type_array = []\n",
    "\n",
    "for strategy in strategies:\n",
    "    feats = opt_results_val[strategy][\"features\"]\n",
    "    for feat_name, feat_dict in feats.items():\n",
    "        feature_set_array.append(strategy)\n",
    "        feature_name_array.append(feat_name)\n",
    "        if \"MEA\" not in feat_name:\n",
    "            feature_type_array.append(\"intra\")\n",
    "        else:\n",
    "            feature_type_array.append(\"extra\")\n",
    "        feature_score_array.append(feat_dict[\"score\"])\n",
    "        protocol_type = feat_name.split(\".\")[0].split(\"_\")[0]\n",
    "        protocol_type_array.append(protocol_type)\n",
    "        \n",
    "df_feats = pd.DataFrame({\"feature_set\": feature_set_array, \"feat_name\": feature_name_array,\n",
    "                         \"feature_type\": feature_type_array, \"feat_score\": feature_score_array, \n",
    "                         \"protocol_type\": protocol_type_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats_intra = df_feats.query(\"feature_type == 'intra'\").dropna()\n",
    "df_feats_extra = df_feats.query(\"feature_type == 'extra'\").dropna()\n",
    "\n",
    "fig_feat_intra, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_feats_intra, x=\"feature_set\", y=\"feat_score\", order=order, #hue=\"protocol_type\", \n",
    "            ax=ax, showfliers=False)\n",
    "n = len(df_feats_intra.query(\"feature_set == 'soma'\"))\n",
    "# g = sns.swarmplot(data=df_feats, y=\"feature_set\", x=\"feat_score\", ax=ax)\n",
    "ax.set_ylabel(\"Feature scores\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(f\"Intracellular features\\n(n={n})\", fontsize=20)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "#ax.set_ylim(0, 21)\n",
    "\n",
    "fig_feat_extra, ax = plt.subplots(figsize=(7, 10))\n",
    "sns.boxplot(data=df_feats_extra, \n",
    "            x=\"feature_set\", y=\"feat_score\", order=order, ax=ax, showfliers=False)\n",
    "n = len(df_feats_extra.query(\"feature_set == 'soma'\"))\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(f\"Extracellular features\\n(n={n})\", fontsize=20)\n",
    "ax.set_xlabel(\"Strategy\", fontsize=15)\n",
    "ax.set_ylabel(\"Score\", fontsize=15)\n",
    "#ax.set_ylim(0, 21)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_feat_intra.savefig(figure_folder / \"fig4Bleft-intra.pdf\")\n",
    "    fig_feat_extra.savefig(figure_folder / \"fig4Bright-extra.pdf\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import statsmodels.api as sa\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.posthoc_conover(df_feats_intra, val_col='feat_score', \n",
    "                   group_col='feature_set', p_adjust = 'holm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.posthoc_conover(df_feats_extra, val_col='feat_score', \n",
    "                   group_col='feature_set', p_adjust = 'holm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intra_soma = df_feats_intra.query(\"feature_set == 'soma'\")[\"feat_score\"]\n",
    "intra_sections = df_feats_intra.query(\"feature_set == 'sections'\")[\"feat_score\"]\n",
    "intra_all = df_feats_intra.query(\"feature_set == 'all'\")[\"feat_score\"]\n",
    "intra_single = df_feats_intra.query(\"feature_set == 'single'\")[\"feat_score\"]\n",
    "\n",
    "extra_soma = df_feats_extra.query(\"feature_set == 'soma'\")[\"feat_score\"]\n",
    "extra_sections = df_feats_extra.query(\"feature_set == 'sections'\")[\"feat_score\"]\n",
    "extra_all = df_feats_extra.query(\"feature_set == 'all'\")[\"feat_score\"]\n",
    "extra_single = df_feats_extra.query(\"feature_set == 'single'\")[\"feat_score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Intra - Sections VS SOMA:\", wilcoxon(intra_sections, intra_soma))\n",
    "# print(\"Intra - All VS SOMA:\", wilcoxon(intra_all, intra_soma))\n",
    "#print(\"Intra - Single VS SOMA:\", wilcoxon(intra_single, intra_soma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Extra - Sections VS SOMA:\", wilcoxon(extra_sections, extra_soma))\n",
    "# print(\"Extra - All VS SOMA:\", wilcoxon(extra_all, extra_soma))\n",
    "# print(\"Extra - Single VS SOMA:\", wilcoxon(extra_single, extra_soma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.DataFrame.from_dict(opt_results_val, orient=\"index\")\n",
    "df_test[\"strategy\"] = df_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_cos, ax = plt.subplots()\n",
    "sns.barplot(data=df_test, x=\"strategy\", y=\"eap_dist\", order=order, ax=ax)\n",
    "ax.set_ylabel(\"Distance\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Extracellular difference\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results_val[\"gt\"] = {}\n",
    "opt_results_val[\"gt\"][\"responses\"] = responses_release_train\n",
    "opt_results_val[\"gt\"][\"eap\"] = eap_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(result_folder / results_val_file, 'wb') as f:\n",
    "    pickle.dump(opt_results_val, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
