{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze and evaluate optimization output\n",
    "\n",
    "This final notebook uses the `runs.pkl` file created in notebook 2 and it analyzes:\n",
    "\n",
    "- the distance between different feature sets in the parameter space\n",
    "- the distance between different feature sets in the feature space\n",
    "- the distance between different feature sets in the extracellular signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "import bluepyopt as bpopt\n",
    "import bluepyopt.ephys as ephys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import distance\n",
    "import MEAutility as mu\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.stats import kruskal, mannwhitneyu, wilcoxon\n",
    "\n",
    "import multimodalfitting as mf\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_fig = False\n",
    "figure_folder = Path(\".\") / \"figures_hay_ais\"\n",
    "\n",
    "if save_fig:\n",
    "    figure_folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path(\"../..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GT params and optimization output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general\n",
    "model_name = \"hay_ais\"\n",
    "probe_type = \"planar\" # linear \n",
    "\n",
    "cell_models_folder = base_dir / \"cell_models\"\n",
    "model_folder = cell_models_folder / model_name\n",
    "probe_file = model_folder / \"fitting\" / \"efeatures\" / \"probe_BPO.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_date = '211124'  #. '220111' # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this with folder containing your pkl file\n",
    "result_folder = base_dir / \"results\" / results_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                    release=False)\n",
    "cell_release = mf.create_ground_truth_model(model_name=model_name,\n",
    "                                            release=True)\n",
    "\n",
    "probe = mf.define_electrode(probe_file=probe_file)\n",
    "\n",
    "param_names = [param.name for param in cell.params.values() if not param.frozen]\n",
    "\n",
    "params_release = {}\n",
    "for param in cell_release.params_by_names(param_names):\n",
    "    params_release[param.name] = param.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocol_for_eap = \"firepattern_200\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file_name = \"runs.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(result_folder / pkl_file_name, 'rb'))\n",
    "df_optimization = pd.DataFrame(data)\n",
    "df_model = df_optimization.query(f\"model == '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_soma = df_model.query(\"feature_set == 'soma'\")\n",
    "opt_extra = df_model.query(\"feature_set == 'extra'\")\n",
    "print(f\"Somatic optimizations: {len(opt_soma)}\")\n",
    "print(f\"Extra optimizations: {len(opt_extra)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "min_evals = 3000\n",
    "color_strategy = {\"all\": \"C1\", \"sections\": \"C2\", \"single\": \"C3\"}\n",
    "\n",
    "keep_idxs_soma = []\n",
    "for idx, row in opt_soma.iterrows():\n",
    "    if max(row[\"nevals\"]) > min_evals:\n",
    "        keep_idxs_soma.append(idx)\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=\"C0\",\n",
    "                ls='-', \n",
    "                lw=0.8,\n",
    "                alpha=0.75)\n",
    "    else:\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=\"C0\",\n",
    "                ls='--', \n",
    "                lw=0.5,\n",
    "                alpha=0.75)\n",
    "keep_idx_extra = []\n",
    "for idx, row in opt_extra.iterrows():\n",
    "    #print(max(row['nevals']))\n",
    "    if max(row[\"nevals\"]) > min_evals:\n",
    "        keep_idx_extra.append(idx)\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=color_strategy[row[\"extra_strategy\"]],\n",
    "                ls='-', \n",
    "                lw=0.8,\n",
    "                alpha=0.75, \n",
    "                label=idx)\n",
    "    else:\n",
    "        ax.plot(row[\"nevals\"], \n",
    "                row[\"logbook\"].select(\"min\"),\n",
    "                color=color_strategy[row[\"extra_strategy\"]],\n",
    "                ls='--', \n",
    "                lw=0.5,\n",
    "                alpha=0.75, \n",
    "                label=idx)\n",
    "ax.set_title(\"Min fitness\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_xlabel(\"Neval\")\n",
    "ax.set_ylabel(\"Min fitness\")\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load protocols and original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_kwargs = mf.utils.get_extra_kwargs()\n",
    "extra_kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protocols_used_for_opt = [\"IV_-20\", \"IV_-100\", \"IDrest_150\", \"IDrest_250\", \"IDrest_300\",\n",
    "                          \"APWaveform_260\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_extra = mf.create_evaluator(\n",
    "    model_name=model_name,\n",
    "    feature_set=\"extra\",\n",
    "    extra_strategy=\"all\",\n",
    "    protocols_with_lfp=\"IDrest_300\",\n",
    "    all_protocols=True,\n",
    "    exclude_protocols=protocols_used_for_opt,\n",
    "    **extra_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"All test features --> num features {len(eva_extra.fitness_calculator.objectives)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute release responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_release = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), param_values=params_release)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_release = {}\n",
    "for obj in eva_extra.fitness_calculator.objectives:\n",
    "    features_release[obj.features[0].name] = {}\n",
    "    if len(obj.features) == 1:\n",
    "        feat_value = obj.features[0].calculate_feature(responses_release)\n",
    "        feat_score = obj.features[0].calculate_score(responses_release)\n",
    "        features_release[obj.features[0].name][\"value\"] = feat_value\n",
    "        features_release[obj.features[0].name][\"score\"] = feat_score\n",
    "    else:\n",
    "        print(f\"More than one feature for objective: {obj.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(list(responses_release.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_gt_intra = mf.plot_responses(responses_release, color=\"k\", return_fig=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_gt_intra.savefig(figure_folder / \"gt_intra.pdf\")\n",
    "    fig_gt_extra.savefig(figure_folder / \"gt_extra.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"soma\" example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = \"soma\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results[strategy] = {}\n",
    "best_idx = np.argmin(opt_soma.best_fitness)\n",
    "params_sample = opt_df.iloc[best_idx]\n",
    "params_dict = {k: v for k, v in zip(param_names, params_sample.best_params)}\n",
    "opt_results[strategy][\"best_fitness\"] = params_sample.best_fitness\n",
    "opt_results[strategy][\"best_params\"] = params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_soma = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), param_values=params_sample_soma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results[strategy][\"responses\"] = response_soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_best_soma = {}\n",
    "for obj in eva_extra.fitness_calculator.objectives:\n",
    "    features_best_soma[obj.features[0].name] = {}\n",
    "    if len(obj.features) == 1:\n",
    "        feat_value = obj.features[0].calculate_feature(response_soma)\n",
    "        feat_score = obj.features[0].calculate_score(response_soma)\n",
    "        features_best_soma[obj.features[0].name][\"value\"] = feat_value\n",
    "        features_best_soma[obj.features[0].name][\"score\"] = feat_score\n",
    "    else:\n",
    "        print(f\"More than one feature for objective: {obj.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results[strategy][\"features\"] = features_best_soma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_soma_intra = mf.plot_multiple_responses([responses_release, response_soma], colors=[\"k\", \"C0\"], \n",
    "                                            labels=[\"GT\", \"SOMA\"], return_fig=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"extra\" example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt_results = {}\n",
    "for extra_strategy in np.unique(opt_extra.extra_strategy):\n",
    "    opt_results[extra_strategy] = {}\n",
    "    opt_df = opt_extra.query(f\"extra_strategy == '{extra_strategy}'\")\n",
    "    best_idx = np.argmin(opt_df.best_fitness)\n",
    "    params_sample = opt_df.iloc[best_idx]\n",
    "    params_dict = {k: v for k, v in zip(param_names, params_sample.best_params)}\n",
    "    opt_results[extra_strategy][\"best_fitness\"] = params_sample.best_fitness\n",
    "    opt_results[extra_strategy][\"best_params\"] = params_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_results[\"soma\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for extra_strategy in np.unique(opt_extra.extra_strategy):\n",
    "    print(f\"Simulating best '{extra_strategy}'\")\n",
    "    responses = eva_extra.run_protocols(eva_extra.fitness_protocols.values(), \n",
    "                                        param_values=opt_results[extra_strategy][\"best_params\"])\n",
    "    opt_results[extra_strategy][\"responses\"] = responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for extra_strategy in np.unique(opt_extra.extra_strategy):\n",
    "    responses = opt_results[extra_strategy][\"responses\"]\n",
    "    features_best = {}\n",
    "    for obj in eva_extra.fitness_calculator.objectives:\n",
    "        features_best[obj.features[0].name] = {}\n",
    "        if len(obj.features) == 1:\n",
    "            feat_value = obj.features[0].calculate_feature(responses)\n",
    "            feat_score = obj.features[0].calculate_score(responses)\n",
    "            features_best[obj.features[0].name][\"value\"] = feat_value\n",
    "            features_best[obj.features[0].name][\"score\"] = feat_score\n",
    "        else:\n",
    "            print(f\"More than one feature for objective: {obj.name}\")\n",
    "    opt_results[extra_strategy][\"features\"] = features_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_dict = {\"soma\": \"C0\",\n",
    "               \"all\": \"C1\",\n",
    "               \"sections\": \"C2\",\n",
    "               \"single\": \"C3\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_list = [responses_release]\n",
    "colors = [\"k\"]\n",
    "labels = [\"GT\"]\n",
    "\n",
    "for strategy, strategy_dict in opt_results.items():\n",
    "    responses_list.append(strategy_dict[\"responses\"])\n",
    "    colors.append(colors_dict[strategy])    \n",
    "    labels.append(strategy.upper())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_extra_intra = mf.plot_multiple_responses(responses_list, \n",
    "                                             colors=colors, return_fig=True, \n",
    "                                             labels=labels)\n",
    "# fig_extra_single_intra = mf.plot_multiple_responses([responses_release, response_single], \n",
    "#                                                      colors=[\"k\", \"C3\"], return_fig=True, labels=[\"GT\", \"SINGLE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_extra_intra.savefig(figure_folder / \"extra_intra.pdf\")\n",
    "    fig_extra_extra.savefig(figure_folder / \"extra_extra.pdf\")\n",
    "    fig_extra_single_intra.savefig(figure_folder / \"single_intra.pdf\")\n",
    "    fig_extra_single.savefig(figure_folder / \"single_extra.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare best-fitted models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_name_array = []\n",
    "feature_set_array = []\n",
    "feature_score_array = []\n",
    "feature_type_array = []\n",
    "protocol_type_array = []\n",
    "\n",
    "for strategy, strategy_dict in opt_results.items():\n",
    "    feats = strategy_dict[\"features\"]\n",
    "    for feat_name, feat_dict in feats.items():\n",
    "        feature_set_array.append(strategy)\n",
    "        feature_name_array.append(feat_name)\n",
    "        if \"MEA\" not in feat_name:\n",
    "            feature_type_array.append(\"intra\")\n",
    "        else:\n",
    "            feature_type_array.append(\"extra\")\n",
    "        feature_score_array.append(feat_dict[\"score\"])\n",
    "        protocol_type = feat_name.split(\".\")[0].split(\"_\")[0]\n",
    "        protocol_type_array.append(protocol_type)\n",
    "        \n",
    "df_feats = pd.DataFrame({\"feature_set\": feature_set_array, \"feat_name\": feature_name_array,\n",
    "                         \"feat_score\": feature_score_array, \"protocol_type\": protocol_type_array})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_feat_intra, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "sns.boxplot(data=df_feats, y=\"feature_set\", x=\"feat_score\",# hue=\"protocol_type\", \n",
    "            ax=ax)\n",
    "# g = sns.swarmplot(data=df_feats, y=\"feature_set\", x=\"feat_score\", ax=ax)\n",
    "ax.set_ylabel(\"Feature scores (intracellular)\", fontsize=12)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.set_title(\"Intracellular features\", fontsize=15)\n",
    "ax.set_xlim([0, 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for protocol_type in np.unique(df_feats.protocol_type):\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.barplot(data=df_feats.query(f\"protocol_type == '{protocol_type}'\"), \n",
    "                y=\"feature_set\", x=\"feat_score\",# hue=\"protocol_type\", \n",
    "                ax=ax)\n",
    "    ax.set_title(protocol_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_fig:\n",
    "    fig_feat_intra.savefig(figure_folder / \"feat_intra.pdf\")\n",
    "    fig_feat_extra.savefig(figure_folder / \"feat_extra.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
