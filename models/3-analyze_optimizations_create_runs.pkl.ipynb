{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze optimization results and create runs.pkl\n",
    "\n",
    "After running the model optimizations with the `run_optimizations.py` script, this notebook checks the optimization outputs and it creates a summary file called `runs.pkl` in the `results` folder.\n",
    "\n",
    "The `runs.pkl` is used in notebook 3 to compare the optimization outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = Path('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkp(path):\n",
    "    \n",
    "    try:\n",
    "        chkp_name = os.path.basename(path)[:-4]\n",
    "        chkp_name = chkp_name.split('_')\n",
    "\n",
    "        feature_set = chkp_name[0]\n",
    "        seed = chkp_name[-1][:1]\n",
    "        \n",
    "        sample_id = path.split(\"/\")[-2]\n",
    "        sample_id = sample_id.strip(\"random_\")\n",
    "\n",
    "        with open(path, 'rb') as fp:\n",
    "            run = pickle.load(fp)\n",
    "        \n",
    "        run = {\"nevals\": numpy.cumsum(run['logbook'].select(\"nevals\")),\n",
    "               \"population\": run['population'],\n",
    "               \"hof\": run['halloffame'],\n",
    "               \"logbook\": run['logbook'],\n",
    "               \"sample_id\": sample_id,\n",
    "               \"seed\": seed,\n",
    "               \"feature_set\": feature_set,\n",
    "               \"best_fitness\": numpy.sum(run['halloffame'][0].fitness.values),\n",
    "               \"best_scores\": list(run['halloffame'][0].fitness.values),\n",
    "               \"best_params\": list(run['halloffame'][0]),\n",
    "               \"path\": path}\n",
    "        \n",
    "        return run\n",
    "        \n",
    "    except:      \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for path in glob.glob('./checkpoints/**/*.pkl', recursive=True):\n",
    "\n",
    "    run = load_checkp(path)\n",
    "    if run:\n",
    "        print(path)\n",
    "        runs.append(run)\n",
    "    else:\n",
    "        print(\"Failed to read \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = set([run[\"sample_id\"] for run in runs]) \n",
    "colors = {i: \"C{}\".format(i) for i in ids}\n",
    "colors_set = {\"extra\": \"C0\", \"bap\": \"C1\", \"soma\": \"C2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors[run[\"sample_id\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors[run[\"sample_id\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_set[run[\"feature_set\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_set[run[\"feature_set\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_ylim(5, 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_selected = []\n",
    "for feature_set in ['extra', 'bap', 'soma']:\n",
    "    \n",
    "    for sample_id in [\"0\", \"1\", \"2\", \"3\", \"4\"]:\n",
    "        \n",
    "        run_set = [run for run in runs if run['feature_set'] == feature_set and run['sample_id'] == sample_id]\n",
    "        fit = [run['best_fitness'] for run in run_set]\n",
    "        \n",
    "        avg = numpy.mean(fit)\n",
    "        \n",
    "\n",
    "        fit, run_set = zip(*sorted(zip(fit, run_set)))\n",
    "        \n",
    "        run_selected = run_set[:5]\n",
    "        \n",
    "        runs_selected += run_selected\n",
    "        \n",
    "        print(feature_set, sample_id, avg, len(run_set))\n",
    "        print(numpy.mean( [run['best_fitness'] for run in run_selected]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs_selected:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_set[run[\"feature_set\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_set[run[\"feature_set\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(ids), figsize=(20,8))\n",
    "\n",
    "for i, sample_id in enumerate(ids):\n",
    "\n",
    "    ordered_runs = list([run for run in runs_selected if run['sample_id'] == sample_id])\n",
    "    ordered_runs = sorted(ordered_runs, key=lambda kv: kv['best_fitness'])\n",
    "    \n",
    "    labels = [run['seed'] for run in ordered_runs]\n",
    "    ytick_pos = [x for x in range(len(labels))]\n",
    "    clrs = [colors_set[run['feature_set']] for run in ordered_runs]\n",
    "    \n",
    "    for pos, fit, es, c in zip(ytick_pos, ordered_runs, labels, clrs):\n",
    "        ax[i].barh([pos],\n",
    "                   [fit['best_fitness']],\n",
    "                   height=0.5,\n",
    "                   align='center',\n",
    "                   color=c,\n",
    "                   alpha=0.8)\n",
    "\n",
    "    ax[i].set_yticks(ytick_pos, [])\n",
    "    ax[i].set_yticklabels(labels, size='large')\n",
    "    ax[i].set_xlabel(\"Fitness\", size=\"x-large\")\n",
    "    ax[i].set_ylim(-1, len(labels))\n",
    "    ax[i].set_title(sample_id)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(results_folder / \"runs.pkl\"), 'wb') as fp:\n",
    "    pickle.dump(runs_selected, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}