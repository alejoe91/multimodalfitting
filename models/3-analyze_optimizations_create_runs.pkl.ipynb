{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Analyze optimization results and create runs.pkl\n",
    "\n",
    "After running the model optimizations with the `run_optimizations.py` script, this notebook checks the optimization outputs and it creates a summary file called `runs.pkl` in the `results` folder.\n",
    "\n",
    "The `runs.pkl` is used in notebook 4 to compare the optimization outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from pathlib import Path\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_folder = Path('optimization_results/checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_checkpoint(checkpoint_path):\n",
    "    \"\"\"Reads a BluePyOpt checkpoint file\"\"\"\n",
    "\n",
    "    p = Path(checkpoint_path)\n",
    "    p_tmp = p.with_suffix(p.suffix + \".tmp\")\n",
    "\n",
    "    try:\n",
    "        run = pickle.load(open(str(p), \"rb\"))\n",
    "    except EOFError:\n",
    "        try:\n",
    "            run = pickle.load(open(str(p_tmp), \"rb\"))\n",
    "        except EOFError:\n",
    "            logger.error(\n",
    "                \"Cannot store model. Checkpoint file %s does not exist or is corrupted.\",\n",
    "                checkpoint_path,\n",
    "            )\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def load_checkp(checkpoint_path):\n",
    "    \n",
    "    run = read_checkpoint(checkpoint_path)\n",
    "    \n",
    "    chkp_name = Path(checkpoint_path).stem\n",
    "\n",
    "    chkp_name_split = chkp_name.split('_')\n",
    "    \n",
    "    model = \"hay\"\n",
    "    if \"hay_ais\" in chkp_name:\n",
    "        model = \"hay_ais\"\n",
    "\n",
    "    feature_set = [e.replace('featureset=', '') for e in chkp_name_split if \"featureset=\" in e ][0]\n",
    "    seed = int([e.replace('seed=', '') for e in chkp_name_split if \"seed=\" in e ][0])\n",
    "\n",
    "    run = {\"nevals\": numpy.cumsum(run['logbook'].select(\"nevals\")),\n",
    "           \"population\": run['population'],\n",
    "           \"hof\": run['halloffame'],\n",
    "           \"logbook\": run['logbook'],\n",
    "           \"model\": model,\n",
    "           \"seed\": seed,\n",
    "           \"feature_set\": feature_set,\n",
    "           \"best_fitness\": numpy.sum(run['halloffame'][0].fitness.values),\n",
    "           \"best_scores\": list(run['halloffame'][0].fitness.values),\n",
    "           \"best_params\": list(run['halloffame'][0]),\n",
    "           \"path\": path}\n",
    "    \n",
    "    return run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "\n",
    "for path in glob.glob(f\"{str(results_folder)}/*\"):\n",
    "    \n",
    "    if \".tmp\" in path or \"runs\" in path:\n",
    "        continue\n",
    "    \n",
    "    print(path)\n",
    "    runs.append(load_checkp(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = set([run[\"model\"] for run in runs])\n",
    "colors_model = {m: \"C{}\".format(i) for i,m in enumerate(models)}\n",
    "print(colors_model)\n",
    "colors_set = {\"extra\": \"C0\", \"bap\": \"C1\", \"soma\": \"C2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_model[run[\"model\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_model[run[\"model\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_ylim(5, 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_set[run[\"feature_set\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_set[run[\"feature_set\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_ylim(5, 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(str(results_folder / \"runs.pkl\"), 'wb') as fp:\n",
    "    pickle.dump(runs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_responses(responses):\n",
    "    fig, axes = plt.subplots(len(responses), figsize=(10,10))\n",
    "    for index, (resp_name, response) in enumerate(sorted(responses.items())):\n",
    "        axes[index].plot(response['time'], response['voltage'], label=resp_name)\n",
    "        axes[index].set_title(resp_name)\n",
    "    fig.tight_layout()\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "\n",
    "    if run[\"feature_set\"] == \"soma\":\n",
    "\n",
    "        probe_type = None\n",
    "        protocols_with_lfp = None\n",
    "        timeout = 300.\n",
    "        extra_kwargs = {}\n",
    "\n",
    "        if run[\"feature_set\"] == \"extra\":\n",
    "            probe_type = \"planar\"\n",
    "            protocols_with_lfp = ['firepattern_200']\n",
    "            timeout = 900.\n",
    "\n",
    "        feature_file = f\"../data/{run['model']}_ecode_probe_planar/efeatures/features_BPO_test.json\"\n",
    "        protocol_file = f\"../data/{run['model']}_ecode_probe_planar/efeatures/protocols_BPO_test.json\"\n",
    "\n",
    "        eva = evaluator.create_evaluator(\n",
    "            model_name=run[\"model\"],\n",
    "            feature_set=run[\"feature_set\"],\n",
    "            feature_file=feature_file,\n",
    "            protocol_file=protocol_file,\n",
    "            probe_type=probe_type,\n",
    "            protocols_with_lfp=protocols_with_lfp,\n",
    "            extra_recordings=None,\n",
    "            timeout=timeout,\n",
    "            fs=20, \n",
    "            fcut=300,\n",
    "            ms_cut=[2, 10],\n",
    "            upsample=10\n",
    "        )\n",
    "        \n",
    "        best_params = eva.param_dict(run[\"best_params\"])\n",
    "        responses = eva.run_protocols(protocols=eva.fitness_protocols.values(), param_values=best_params)\n",
    "        \n",
    "        plot_responses(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
