{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from analyse_log import analyse_log\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import glob\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "for fl in glob.glob('./logs/*.stdout'):\n",
    "#for fl in glob.glob('/gpfs/bbp.cscs.ch/data/project/.snapshots/proj_daily-2020.10.08-03.49.12/proj38/home/damart/LFPy/multimodalfitting/l5pc_multimodal/logs/*.stdout'):\n",
    "    \n",
    "    path_params = fl[:-7] + '/params.dat'\n",
    "    with open(path_params, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "    sample_id = int(lines[2])\n",
    "        \n",
    "    gen = analyse_log(fl)\n",
    "    \n",
    "    meann = [numpy.mean(g['tasks']) for g in gen][:-1]\n",
    "    axs[0].plot(\n",
    "        [i for i in range(len(meann))],\n",
    "        meann,\n",
    "        #yerr=[numpy.std(g['tasks']) for g in gen],\n",
    "        alpha=0.6,\n",
    "        color=f\"C{sample_id}\",\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        maxx = [numpy.max(g['tasks']) for g in gen if len(g)][:-1]\n",
    "        \n",
    "        axs[1].plot(\n",
    "            [i for i in range(len(maxx))],\n",
    "            maxx,\n",
    "            alpha=0.6,\n",
    "            color=f\"C{sample_id}\"\n",
    "        )\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "for i in range(5):\n",
    "    axs[0].plot([20, 20], [300, 300], color=f\"C{i}\", label=f\"Sample id {i}\")\n",
    "\n",
    "axs[0].set_xlabel('Generation', size='x-large')\n",
    "axs[0].set_ylabel('Mean compute time per task (s)', size='x-large')\n",
    "axs[0].legend()\n",
    "axs[1].set_xlabel('Generation', size='x-large')\n",
    "axs[1].set_ylabel('Run time of generation (s)', size='x-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import glob\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import l5pc_model\n",
    "import l5pc_evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkp(path):\n",
    "    \n",
    "    try:\n",
    "        chkp_name = os.path.basename(path)[:-4]\n",
    "        chkp_name = chkp_name.split('_')\n",
    "\n",
    "        feature_set = chkp_name[0]\n",
    "        seed = chkp_name[-1][:1]\n",
    "        \n",
    "        sample_id = path.split(\"/\")[-2]\n",
    "        sample_id = sample_id.strip(\"random_\")\n",
    "\n",
    "        with open(path, 'rb') as fp:\n",
    "            run = pickle.load(fp)\n",
    "        \n",
    "        run = {\"nevals\": numpy.cumsum(run['logbook'].select(\"nevals\")),\n",
    "               \"population\": run['population'],\n",
    "               \"hof\": run['halloffame'],\n",
    "               \"logbook\": run['logbook'],\n",
    "               \"sample_id\": sample_id,\n",
    "               \"seed\": seed,\n",
    "               \"feature_set\": feature_set,\n",
    "               \"best_fitness\": numpy.sum(run['halloffame'][0].fitness.values),\n",
    "               \"best_scores\": list(run['halloffame'][0].fitness.values),\n",
    "               \"best_params\": list(run['halloffame'][0]),\n",
    "               \"path\": path}\n",
    "        \n",
    "        return run\n",
    "        \n",
    "    except:      \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for path in glob.glob('/gpfs/bbp.cscs.ch/data/project/.snapshots/proj_daily-2020.10.08-03.49.12/proj38/home/damart/LFPy/multimodalfitting/l5pc_multimodal/checkpoints/**/*.pkl', recursive=True):\n",
    "\n",
    "    run = load_checkp(path)\n",
    "    if run:\n",
    "        print(path)\n",
    "        runs.append(run)\n",
    "    else:\n",
    "        print(\"Failed to read \", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = set([run[\"sample_id\"] for run in runs]) \n",
    "colors = {i: \"C{}\".format(i) for i in ids}\n",
    "colors_set = {\"extra\": \"C0\", \"bap\": \"C1\", \"soma\": \"C2\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors[run[\"sample_id\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors[run[\"sample_id\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_set[run[\"feature_set\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_set[run[\"feature_set\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "#ax.set_ylim(5, 20)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_selected = []\n",
    "for feature_set in ['extra', 'bap', 'soma']:\n",
    "    \n",
    "    for sample_id in [\"0\", \"1\", \"2\", \"3\", \"4\"]:\n",
    "        \n",
    "        run_set = [run for run in runs if run['feature_set'] == feature_set and run['sample_id'] == sample_id]\n",
    "        fit = [run['best_fitness'] for run in run_set]\n",
    "        \n",
    "        avg = numpy.mean(fit)\n",
    "        print(feature_set, sample_id, avg, len(run_set))\n",
    "\n",
    "        fit, run_set = zip(*sorted(zip(fit, run_set)))\n",
    "        \n",
    "        run_selected = run_set[:5]\n",
    "        print(numpy.mean( [run['best_fitness'] for run in run_selected]))\n",
    "        print()\n",
    "        #for run in runs:\n",
    "        runs_selected += run_selected\n",
    "        \n",
    "runs = runs_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "for run in runs:\n",
    "\n",
    "    ax.plot(run[\"nevals\"], \n",
    "            run[\"logbook\"].select(\"min\"),\n",
    "            color=colors_set[run[\"feature_set\"]],\n",
    "            ls='--', \n",
    "            lw=0.5,\n",
    "            alpha=0.75)\n",
    "    \n",
    "    ax.scatter([run[\"nevals\"][-1]], \n",
    "               [numpy.sum(run[\"hof\"][0].fitness.values)],\n",
    "               color=colors_set[run[\"feature_set\"]],\n",
    "               alpha=0.75)\n",
    "    \n",
    "ax.set_xlabel(\"Number of evaluations\", size=\"x-large\")\n",
    "ax.set_ylabel(\"Minimum fitness (std)\", size=\"x-large\")\n",
    "\n",
    "ax.set_yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(ids), figsize=(20,8))\n",
    "\n",
    "for i, sample_id in enumerate(ids):\n",
    "\n",
    "    ordered_runs = list([run for run in runs if run['sample_id'] == sample_id])\n",
    "    ordered_runs = sorted(ordered_runs, key=lambda kv: kv['best_fitness'])\n",
    "    \n",
    "    labels = [run['seed'] for run in ordered_runs]\n",
    "    ytick_pos = [x for x in range(len(labels))]\n",
    "    clrs = [colors_set[run['feature_set']] for run in ordered_runs]\n",
    "    \n",
    "    for pos, fit, es, c in zip(ytick_pos, ordered_runs, labels, clrs):\n",
    "        ax[i].barh([pos],\n",
    "                   [fit['best_fitness']],\n",
    "                   height=0.5,\n",
    "                   align='center',\n",
    "                   color=c,\n",
    "                   alpha=0.8)\n",
    "\n",
    "    ax[i].set_yticks(ytick_pos, [])\n",
    "    ax[i].set_yticklabels(labels, size='large')\n",
    "    ax[i].set_xlabel(\"Fitness\", size=\"x-large\")\n",
    "    ax[i].set_ylim(-1, len(labels))\n",
    "    ax[i].set_title(sample_id)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the target random params for all sample_ids\n",
    "random_params_file = 'config/params/random.csv'\n",
    "random_params = pd.read_csv(random_params_file, index_col='index')\n",
    "gt_params = {i: random_params.iloc[int(i)].to_dict() for i in ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an evaluator to each run and compute the distance to the original parmeter set\n",
    "for i, run in enumerate(runs):\n",
    "    \n",
    "    prep = l5pc_evaluator.prepare_optimization(\n",
    "        feature_set=run['feature_set'], \n",
    "        sample_id=run['sample_id'], \n",
    "        offspring_size=1, \n",
    "        channels='map',\n",
    "        probe_type='linear',                             \n",
    "        map_function = None)\n",
    "\n",
    "    runs[i]['params_name'] = prep['evaluator'].param_names\n",
    "\n",
    "    original_params = numpy.asarray([gt_params[run[\"sample_id\"]][k] for k in prep['evaluator'].param_names])\n",
    "    fitted_params = numpy.asarray(list(run[\"hof\"][0]))\n",
    "    runs[i][\"distance\"] = cosine(original_params, fitted_params)\n",
    "    runs[i][\"population_distance\"] = [\n",
    "        cosine(original_params, numpy.asarray(list(ind))) for ind in runs[i]['population']\n",
    "    ]\n",
    "        \n",
    "    original_params = numpy.asarray([gt_params[run[\"sample_id\"]][k] for k in prep['evaluator'].param_names if 'apical' in k])\n",
    "    fitted_params = numpy.asarray([run[\"hof\"][0][j] for j, k in enumerate(prep['evaluator'].param_names) if 'apical' in k])\n",
    "    runs[i][\"apical_distance\"] = cosine(original_params, fitted_params)\n",
    "    runs[i][\"population_distance_apical\"] = [\n",
    "        cosine(original_params, numpy.asarray([ind[j] for j, k in enumerate(prep['evaluator'].param_names) if 'apical' in k])) for ind in runs[i]['population']\n",
    "    ]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error between optimization and original params\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for sample_id in range(0, 5):\n",
    "    \n",
    "    errors = {'extra': [], 'soma': [], 'bap': []} \n",
    "    errors_apical = {'extra': [], 'soma': [], 'bap': []}\n",
    "    for run in runs:\n",
    "        if int(run['sample_id']) == sample_id:\n",
    "            errors[run['feature_set']].append(run['distance'])\n",
    "            errors_apical[run['feature_set']].append(run['apical_distance'])\n",
    "            print(run['apical_distance'], numpy.min(run['population_distance_apical']))\n",
    "    for i, (k, v) in enumerate(errors.items()):\n",
    "        axs[0].bar([(0.7*i)+(sample_id*0.1)], \n",
    "                [numpy.mean(v)], \n",
    "                width=0.1,\n",
    "                color=\"C{}\".format(sample_id),\n",
    "                alpha=0.5)\n",
    "\n",
    "    for i, (k, v) in enumerate(errors_apical.items()):\n",
    "            axs[1].bar([(0.7*i)+(sample_id*0.1)], \n",
    "                    [numpy.mean(v)], \n",
    "                    width=0.1,\n",
    "                    color=\"C{}\".format(sample_id),\n",
    "                    alpha=0.5)\n",
    "\n",
    "# Difference between the five original sample_id for reference\n",
    "dist = []\n",
    "dist_apical = []\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        if i > j:\n",
    "            dist.append(cosine(list(gt_params[str(i)].values()), \n",
    "                               list(gt_params[str(j)].values())))\n",
    "            p1 = [p for n, p in gt_params[str(i)].items() if 'apical' in n]\n",
    "            p2 = [p for n, p in gt_params[str(j)].items() if 'apical' in n]\n",
    "            dist_apical.append(cosine(p1, p2))           \n",
    "axs[0].plot([-0.07, 1.87], [numpy.mean(dist), numpy.mean(dist)], ls='--', lw=0.7, c='black', label='Mean inter-sample_id')\n",
    "axs[1].plot([-0.07, 1.87], [numpy.mean(dist_apical), numpy.mean(dist_apical)], ls='--', lw=0.7, c='black')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks(ticks=[0.2, 0.2+0.7, 0.2+1.4])\n",
    "    ax.set_xticklabels(labels=['extra', 'soma', 'bAP'], size='large')\n",
    "    ax.set_ylim(0., 0.3)\n",
    "    ax.set_xlim(-0.07, 1.87)\n",
    "\n",
    "axs[0].set_ylabel(\"Cosine Distance\", size='large')\n",
    "axs[0].set_title('All parameters', size='large')\n",
    "axs[1].set_title('Apical parameters', size='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cosine.pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute error between optimization and original params\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for sample_id in range(0, 5):\n",
    "    \n",
    "    errors = {'extra': [], 'soma': [], 'bap': []} \n",
    "    errors_apical = {'extra': [], 'soma': [], 'bap': []}\n",
    "    for run in runs:\n",
    "        if int(run['sample_id']) == sample_id:\n",
    "            errors[run['feature_set']].append(numpy.min(run['population_distance']))\n",
    "            errors_apical[run['feature_set']].append(numpy.min(run['population_distance_apical']))\n",
    "            \n",
    "    for i, (k, v) in enumerate(errors.items()):\n",
    "        axs[0].bar([(0.7*i)+(sample_id*0.1)], \n",
    "                [numpy.mean(v)], \n",
    "                width=0.1,\n",
    "                color=\"C{}\".format(sample_id),\n",
    "                alpha=0.5)\n",
    "\n",
    "    for i, (k, v) in enumerate(errors_apical.items()):\n",
    "            axs[1].bar([(0.7*i)+(sample_id*0.1)], \n",
    "                    [numpy.mean(v)], \n",
    "                    width=0.1,\n",
    "                    color=\"C{}\".format(sample_id),\n",
    "                    alpha=0.5)\n",
    "\n",
    "# Difference between the five original sample_id for reference\n",
    "dist = []\n",
    "dist_apical = []\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        if i > j:\n",
    "            dist.append(cosine(list(gt_params[str(i)].values()), \n",
    "                               list(gt_params[str(j)].values())))\n",
    "            p1 = [p for n, p in gt_params[str(i)].items() if 'apical' in n]\n",
    "            p2 = [p for n, p in gt_params[str(j)].items() if 'apical' in n]\n",
    "            dist_apical.append(cosine(p1, p2))           \n",
    "axs[0].plot([-0.07, 1.87], [numpy.mean(dist), numpy.mean(dist)], ls='--', lw=0.7, c='black', label='Mean inter-sample_id')\n",
    "axs[1].plot([-0.07, 1.87], [numpy.mean(dist_apical), numpy.mean(dist_apical)], ls='--', lw=0.7, c='black')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xticks(ticks=[0.2, 0.2+0.7, 0.2+1.4])\n",
    "    ax.set_xticklabels(labels=['extra', 'soma', 'bAP'], size='large')\n",
    "    ax.set_ylim(0., 0.3)\n",
    "    ax.set_xlim(-0.07, 1.87)\n",
    "\n",
    "axs[0].set_ylabel(\"Cosine Distance\", size='large')\n",
    "axs[0].set_title('All parameters', size='large')\n",
    "axs[1].set_title('Apical parameters', size='large')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('cosine.pdf', dpi=200, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_out = runs[:]\n",
    "for i in range(len(runs_out)):\n",
    "    runs_out[i].pop('evaluator', None)\n",
    "    runs_out[i].pop('objectives_calculator', None)\n",
    "    runs_out[i].pop('protocols', None)\n",
    "    runs_out[i].pop('evaluator', None)\n",
    "\n",
    "with open(\"runs.pkl\", 'wb') as fp:\n",
    "    pickle.dump(runs_out, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
